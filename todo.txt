Next steps:
- write canned initial messages (to populate message store on a cold start). Will need to do a little piping to dynamically insert fake timestamps. I'm hoping this will help encourage appropriate tool use and appropriate message length/tone.
  - but might have to rewrite these if the scaffolding for sched/recurring messages changes? unless we do something clever
- maybe write a few test cases in the form of short message histories corresponding to certain desired behaviours or scenarios. e.g.
  - cold wakeup message
  - scheduled message writing
  - scheduling a message when appropriate
- implement Jules.emit_scheduled_message method
  - at the same time, maybe think about what approach to take for prompting recurring wakeup messages. I think the current approach of injecting a fake user message leaves something to be desired. Maybe a prefill instead? With <thinking>? Or insert a fake assistant message? I'm even kind of interested in what happens if I use the "system" role for something like this (it's sort of unclear to me whether this role is supposed to be used for anything other than the initial system prompt)
- do some local testing (trying different models)

- should I start writing unit tests...? (some trickiness with mocking up message store, scheduled messages db, and llm)
- run send_pending_scheduled_messages.py periodically (might have to pay for basic pythonanywhere plan)
- scaffolding for prompting the model to form a scheduled message
- should I catch exceptions in app.py and send them stringified to the telegram chat? Would actually be v useful for debugging.
- is there any reason I can't test locally? Would be a lot easier than dealing with frequently deploying, restarting, reading error logs, etc.
  - even if I can't get it running e2e (need to have some public url that telegram can ping), can at least locally test the server <-> llm stuff.
- play with temperature etc. (not even sure how these are set by default)
- figure out how to properly emit logging messages such that I can read them in the pythonanywhere web ui
- Claude (or at least the first Claude model I tested) doesn't let you use a prefill when sending tools. That's a bit annoying. But it also makes me wonder whether other models might have soft issues with this. If they are disinclined to emit tool_use messages that also have text content, then the prefill might cause them to never or relatively rarely use tools.


- maybe it would be helpful to write a short doc outlining the flow of different... flows. e.g. what happens when the user sends a msg in chat, what happens when a recurring morning message is triggered, what happens when the model tries to set a scheduled message, what happens when we do a pending scheduled messages sweep

Okay, so here's a flow in sort of pseudocode

def query_llm
  query = concatenate together...
    - system prompt
    - goals doc
    - last N messages from the message history...
      - where each message has been translated to the form we want to show the LLM (this mostly entails adding timestamps to the text?)
    - prefill for the next message (timestamp)
  response = llm_api_call(query)
  add response to message store
  if response has a tool invocation:
    add it to the scheduled messages db
  # Optional/not currently implemented: translate response into user-facing text (most important for tool call)
  send response to telegram chat

def send_wakeup_msg:
  add a fake wakeup preamble message to message history
  call query_llm()

def check_scheduled_messages:
  pending = query scheduled msg db for any pending messages
  for each pending msg:
    add a fake scheduled message preamble
    call query_llm()
  
